import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import io
import time
from datetime import datetime

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="Stock Monthly Low Data Extractor",
    page_icon="ğŸ“ˆ",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
st.markdown("""
    <style>
    .main-title {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f2937;
        margin-bottom: 0.5rem;
    }
    .subtitle {
        font-size: 1rem;
        color: #6b7280;
        margin-bottom: 2rem;
    }
    .input-section {
        background-color: #f9fafb;
        padding: 2rem;
        border-radius: 0.5rem;
        margin-bottom: 2rem;
    }
    .success-message {
        color: #10b981;
        font-weight: bold;
    }
    .error-message {
        color: #ef4444;
        font-weight: bold;
    }
    </style>
""", unsafe_allow_html=True)

# ã‚¿ã‚¤ãƒˆãƒ«
st.markdown('<div class="main-title">ğŸ“Š Stock Monthly Low Data Extractor</div>', unsafe_allow_html=True)
st.markdown('<div class="subtitle">è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ã€æœˆé–“å®‰å€¤ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•æŠ½å‡º</div>', unsafe_allow_html=True)

def scrape_kabutan_data(code: str, start_date: datetime, end_date: datetime):
    """æ ªæ¢ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹é–¢æ•°"""
    all_data = []
    page = 1
    max_pages = 10
    
    while page <= max_pages:
        try:
            url = f"https://kabutan.jp/stock/kabuka?code={code}&ashi=mon&page={page}"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                st.error(f"âŒ ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}")
                break
            
            soup = BeautifulSoup(response.content, 'html.parser')
            tables = soup.find_all('table')
            
            if len(tables) < 6:
                break
            
            # Table 4ã¨5ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            page_data = []
            for table_idx in [4, 5]:
                if table_idx < len(tables):
                    table = tables[table_idx]
                    rows = table.find_all('tr')
                    
                    for row in rows:
                        ths = row.find_all('th')
                        tds = row.find_all('td')
                        
                        if len(ths) == 1 and len(tds) == 7:
                            date_str = ths[0].get_text(strip=True)
                            low_str = tds[2].get_text(strip=True).replace(',', '')
                            
                            # æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ãƒã‚§ãƒƒã‚¯
                            if '/' in date_str and len(date_str) == 8:
                                try:
                                    yy, mm, dd = map(int, date_str.split('/'))
                                    year = 2000 + yy
                                    dt = datetime(year, mm, dd)
                                    
                                    if start_date <= dt <= end_date:
                                        page_data.append({
                                            'date': dt.strftime('%Y/%m/%d'),
                                            'low': int(low_str)
                                        })
                                except (ValueError, TypeError):
                                    continue
            
            if not page_data:
                break
            
            all_data.extend(page_data)
            page += 1
            time.sleep(1)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
            
        except Exception as e:
            st.warning(f"âš ï¸ ãƒšãƒ¼ã‚¸ {page} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            break
    
    return all_data

def get_company_name(code: str):
    """ä¼æ¥­åã‚’å–å¾—ã™ã‚‹é–¢æ•°"""
    try:
        url = f"https://kabutan.jp/stock/?code={code}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ä¼æ¥­åã‚’å–å¾—ï¼ˆè¤‡æ•°ã®æ–¹æ³•ã‚’è©¦ã™ï¼‰
        title = soup.find('title')
        if title:
            title_text = title.get_text()
            # ã€Œä¼æ¥­åï¼ˆä¼æ¥­åï¼‰ã€ã‚³ãƒ¼ãƒ‰ã€‘ã€å½¢å¼ã‹ã‚‰ä¼æ¥­åã‚’æŠ½å‡º
            if 'ã€' in title_text:
                company_name = title_text.split('ã€')[0].strip().split('ï¼ˆ')[0].strip()
                return company_name
        
        return f"Stock_{code}"
    except:
        return f"Stock_{code}"

# å…¥åŠ›ã‚»ã‚¯ã‚·ãƒ§ãƒ³
st.markdown('<div class="input-section">', unsafe_allow_html=True)

col1, col2 = st.columns(2)
with col1:
    code = st.text_input(
        "è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰",
        placeholder="ä¾‹: 1443",
        max_chars=4,
        help="4æ¡ã®è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
    )

with col2:
    st.write("")
    st.write("")
    if st.button("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—", use_container_width=True):
        if not code or len(code) != 4 or not code.isdigit():
            st.error("âŒ æœ‰åŠ¹ãª4æ¡ã®è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        else:
            st.session_state.code = code

st.markdown('</div>', unsafe_allow_html=True)

# æ—¥ä»˜ç¯„å›²è¨­å®š
col1, col2 = st.columns(2)
with col1:
    start_date = st.date_input(
        "é–‹å§‹æ—¥",
        value=datetime(2019, 1, 1),
        help="ãƒ‡ãƒ¼ã‚¿å–å¾—ã®é–‹å§‹æ—¥ã‚’é¸æŠ"
    )
with col2:
    end_date = st.date_input(
        "çµ‚äº†æ—¥",
        value=datetime.now(),
        help="ãƒ‡ãƒ¼ã‚¿å–å¾—ã®çµ‚äº†æ—¥ã‚’é¸æŠ"
    )

# ãƒ‡ãƒ¼ã‚¿å–å¾—å‡¦ç†
if 'code' in st.session_state:
    code = st.session_state.code
    
    with st.spinner("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­..."):
        # ä¼æ¥­åã‚’å–å¾—
        company_name = get_company_name(code)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        data = scrape_kabutan_data(code, start_date, end_date)
    
    if data:
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
        df = pd.DataFrame(data)
        df = df.sort_values('date').reset_index(drop=True)
        
        st.success(f"âœ… {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        st.subheader("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        st.dataframe(df, use_container_width=True, hide_index=True)
        
        # Excelå‡ºåŠ›
        st.subheader("ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        
        # Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¡ãƒ¢ãƒªã«ä½œæˆ
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='æœˆé–“å®‰å€¤', index=False)
        output.seek(0)
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
        filename = f"{company_name}_{code}_æœˆé–“å®‰å€¤.xlsx"
        st.download_button(
            label="ğŸ“¥ Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=output.getvalue(),
            file_name=filename,
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True
        )
        
        # çµ±è¨ˆæƒ…å ±
        st.subheader("ğŸ“Š çµ±è¨ˆæƒ…å ±")
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("æœ€é«˜å€¤", f"Â¥{df['low'].max()}")
        with col2:
            st.metric("æœ€å®‰å€¤", f"Â¥{df['low'].min()}")
        with col3:
            st.metric("å¹³å‡å€¤", f"Â¥{df['low'].mean():.0f}")
        with col4:
            st.metric("ãƒ‡ãƒ¼ã‚¿ä»¶æ•°", len(df))
        
        # å‰Šé™¤
        del st.session_state.code
    else:
        st.error(f"âŒ è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ {code} ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        if 'code' in st.session_state:
            del st.session_state.code

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("---")
st.markdown("""
    <div style="text-align: center; color: #9ca3af; font-size: 0.875rem;">
    <p>ğŸ“Š Stock Monthly Low Data Extractor | ãƒ‡ãƒ¼ã‚¿å‡ºå…¸: <a href="https://kabutan.jp" target="_blank">æ ªæ¢</a></p>
    <p>ã“ã®ãƒ„ãƒ¼ãƒ«ã¯æ•™è‚²ç›®çš„ã§ä½œæˆã•ã‚Œã¦ã„ã¾ã™ã€‚æŠ•è³‡åˆ¤æ–­ã®å‚è€ƒã«ã¯ã—ãªã„ã§ãã ã•ã„ã€‚</p>
    </div>
""", unsafe_allow_html=True)
